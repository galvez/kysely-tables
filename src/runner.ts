import minimist, { ParsedArgs } from 'minimist'
import { log, confirm, spinner, isCancel } from '@clack/prompts'

import pc from 'picocolors'
import { kebabCase, upperFirst } from 'scule'
import { Kysely, KyselyConfig, CompiledQuery } from 'kysely'
import Postgrator from 'postgrator'
import { Pool as PostgresDriver } from 'pg'
import SqliteDriver from 'better-sqlite3'

import { performance as perf } from 'node:perf_hooks'
import {
  existsSync,
  mkdirSync,
  readdirSync,
  readFileSync,
  writeFileSync,
} from 'node:fs'
import { join, parse } from 'node:path'
import {
  createSQLSchemaFromSource,
  createSQLSchemaResetFromSource,
  createSQLSchemaRevision,
} from './index'
import type { DatabaseDriver, SchemaRevisionStatement } from './types'

type Spinner = {
  start: (msg?: string) => void
  stop: (msg?: string, code?: number) => void
  message: (msg?: string) => void
}

const start = perf.now()

type CreateDatabaseOptions = {
  driver: DatabaseDriver
  config: KyselyConfig
  name?: string
}

export function createDatabase<Database>({
  driver,
  config,
  name,
}: CreateDatabaseOptions): Kysely<Database> {
  const argv = minimist(process.argv.slice(1))

  const database = new Kysely<Database>(config)

  if (argv.create) {
    createSchema<Database>(argv, database)
  }

  if (argv.reset) {
    resetSchema<Database>(argv, database)
  }

  if (argv.revision) {
    createSchemaRevision(
      {
        driver,
        config,
        name,
      },
      argv,
      driver,
    )
  }

  return database
}

async function createSchema<Database>(
  argv: ParsedArgs,
  database: Kysely<Database>,
  reset?: boolean,
  activeSpinner?: Spinner,
) {
  const { source, sourceDir, sourceFileName, sourceBaseFileName } = readSource(
    argv._[0],
  )

  const snapshotSchemaFileName = `${sourceBaseFileName}.snapshot.ts`
  const snapshotSchemaFilePath = join(sourceDir, snapshotSchemaFileName)
  if (!reset && existsSync(snapshotSchemaFilePath)) {
    log.warn(
      pc.yellow(
        `${pc.cyan(
          snapshotSchemaFileName,
        )} exists, did you mean to ${pc.magenta('--reset')}?`,
      ),
    )
    console.log()
    process.exit(1)
  }

  const generatedSchema = createSQLSchemaFromSource({
    source,
    fileName: sourceFileName,
  })

  const generatedSchemaFileName = `${sourceBaseFileName}.sql`
  const generatedSchemaFilePath = join(sourceDir, generatedSchemaFileName)
  writeFileSync(generatedSchemaFilePath, generatedSchema.join('\n\n'))

  if (activeSpinner) {
    activeSpinner.message(timed(`Created ${pc.cyan(generatedSchemaFileName)}`))
  } else {
    log.success(timed(`Created ${pc.cyan(generatedSchemaFileName)}`))
  }

  let shouldApply = reset ?? argv.apply
  if (!shouldApply) {
    const apply = await confirm({
      message: reset ? 'Reset schema?' : 'Apply new schema?',
    })
    if (apply && !isCancel(apply)) {
      shouldApply = true
    }
  }

  if (shouldApply) {
    {
      if (!activeSpinner) {
        activeSpinner = spinner()
        activeSpinner.start('Updating database')
      }
      const start = perf.now()
      try {
        await database.transaction().execute(async (trx) => {
          for (const tableSchema of generatedSchema) {
            const query = CompiledQuery.raw(tableSchema)
            await trx.executeQuery(query)
          }
        })
      } catch (err) {
        console.log(generatedSchema)
        console.log(err)
        process.exit(1)
      }

      activeSpinner.stop(timed(`Database updated`, start))
    }

    {
      const start = perf.now()
      writeFileSync(
        snapshotSchemaFilePath,
        '// This file was autogenerated by kysely-tables as\n' +
          '// a means of diffing against local changes in the main file\n' +
          '// for automatically generating schema revisions (migrations)\n\n' +
          source,
      )
      log.success(timed(`Created ${pc.cyan(snapshotSchemaFileName)}`, start))
    }
  }
  console.log()
}

async function resetSchema<Database>(
  argv: ParsedArgs,
  database: Kysely<Database>,
) {
  const { source, sourceFileName } = readSource(argv._[0])

  let shouldApply = argv.apply
  if (!shouldApply) {
    log.warn('This will drop all your database tables.')
    const apply = await confirm({
      message: 'Reset database schema?',
    })
    if (apply && !isCancel(apply)) {
      shouldApply = true
    }
  }

  if (shouldApply) {
    const s = spinner()
    s.start('Updating database')
    try {
      await database.transaction().execute(async (trx) => {
        for (const tableSchemaReset of createSQLSchemaResetFromSource({
          source,
          fileName: sourceFileName,
        })) {
          if (tableSchemaReset.sql) {
            const query = CompiledQuery.raw(tableSchemaReset.sql)
            await trx.executeQuery(query)
          }
        }
      })
    } catch (err: unknown) {
      if (err instanceof Error && err.message) {
        s.stop(pc.bold(pc.redBright('Cancelled.')))
        log.message()
        log.error(pc.redBright(`${upperFirst(err.message)}.`))
      } else {
        s.stop(pc.bold(pc.redBright('Cancelled.')))
        log.message()
        log.error(pc.redBright(`An unknown error ocurred: ${err!.toString()}.`))
      }
      process.exit(1)
    }

    await createSchema(argv, database, true, s)
  }
}

async function createSchemaRevision(
  options: CreateDatabaseOptions,
  argv: ParsedArgs,
  driver: DatabaseDriver,
) {
  const { source, sourceDir, sourceFileName, sourceBaseFileName } = readSource(
    argv._[0],
  )

  const snapshotFileName = `${sourceBaseFileName}.snapshot.ts`
  const snapshotFilePath = join(sourceDir, snapshotFileName)
  if (!existsSync(snapshotFilePath)) {
    log.warn(
      pc.yellow(
        `${pc.cyan(snapshotFileName)} doesn't exist, ${pc.magenta(
          '--create',
        )} or ${pc.magenta('--reset')} your database first.`,
      ),
    )
    console.log()
    process.exit(1)
  }

  const snapshotSource = readFileSync(snapshotFilePath, 'utf8')
  const revision = createSQLSchemaRevision({
    source,
    snapshotSource,
    snapshotFileName,
    fileName: sourceFileName,
  })

  console.log(revision)

  if (!revision.up.length) {
    log.warn(`No changes detected.`)
    process.exit()
  }

  for (const rev of revision.up) {
    if (rev.invalid && rev.invalid.length > 0) {
      for (const { key, message } of rev.invalid) {
        log.error(
          pc.redBright(
            `Couldn\'t create revision for column ${pc.cyan(key)}.\n${message
              .split('\n')
              .map(pc.yellowBright)
              .join('\n')}`,
          ),
        )
      }
      process.exit(1)
    }
    if (rev.warning) {
      if (rev.sql) {
        log.info(rev.sql.split('\n').map(pc.whiteBright).join('\n'))
      }
      log.warn(rev.warning)
    } else {
      if (rev.sql) {
        log.info(rev.sql.split('\n').map(pc.whiteBright).join('\n'))
      }
    }
  }

  writeRevision(sourceDir, argv.revision, revision.up, 'do')

  for (const rev of revision.down) {
    if (rev.invalid && rev.invalid.length > 0) {
      for (const { key, message } of rev.invalid) {
        log.error(
          pc.redBright(
            `Couldn\'t create revision for column ${pc.cyan(key)}.\n${message
              .split('\n')
              .map(pc.yellowBright)
              .join('\n')}`,
          ),
        )
      }
      process.exit(1)
    }
    if (rev.sql) {
      log.info(rev.sql.split('\n').map(pc.whiteBright).join('\n'))
    }
  }

  writeRevision(sourceDir, argv.revision, revision.down, 'undo')

  let shouldApply = argv.apply
  if (!shouldApply) {
    const apply = await confirm({
      message: 'Apply schema revision?',
    })
    if (apply && !isCancel(apply)) {
      shouldApply = true
    }
  }

  if (shouldApply) {
    const client = await getPostgratorClient(driver)

    if (client) {
      const postgrator = new Postgrator({
        migrationPattern: join(sourceDir, 'revisions'),
        driver: client.driver as 'pg' | 'sqlite3',
        ...(client.driver === 'sqlite3' && {
          betterSqlite3: true,
        }),
        database: options.name,
        schemaTable: 'schemaversion',
        execQuery: client.execQuery,
      })
      await postgrator.migrate()
      log.success(timed(`Database updated`, perf.now()))
      process.exit()
    } else {
      log.error('An error occured connection to the database.')
      console.log()
      process.exit(1)
    }
  }
}

function readSource(sourceFilePath: string): Record<string, string> {
  const { name, ext, dir } = parse(sourceFilePath)
  const snapshoptFileName = `${name}.snapshot${ext}`
  const snapshoptFilePath = join(dir, snapshoptFileName)
  const source = readFileSync(sourceFilePath, 'utf8')
  return {
    sourceDir: dir,
    sourceFileName: `${name}${ext}`,
    sourceFilePath,
    sourceBaseFileName: name,
    snapshoptFileName,
    snapshoptFilePath,
    source,
  }
}

function writeRevision(
  sourceDir: string,
  name: string | boolean,
  revision: SchemaRevisionStatement[],
  type: 'do' | 'undo',
) {
  const revisionsDir = join(sourceDir, 'revisions')
  if (!existsSync(revisionsDir)) {
    mkdirSync(revisionsDir, { recursive: true })
    log.warn(
      `Created ${pc.cyan('revisions')} directory, add it to source control.`,
    )
  }
  const revisions = [
    ...new Set(
      readdirSync(revisionsDir)
        .filter((_) => _.match(/\d\d\d\.do/))
        .map((_) => _.slice(0, 3)),
    ),
  ].sort()
  const nextRevision = String(
    revisions.length ? parseInt(revisions.at(-1)!) : '001',
  ).padStart(3, '0')

  const revisionName =
    name === true
      ? new Date().getTime()
      : kebabCase((name as string).replace(/\s+/g, '-'))
  const doRevisionFileName = `${nextRevision}.${type}.${revisionName}.sql`

  {
    const start = perf.now()
    writeFileSync(
      join(revisionsDir, doRevisionFileName),
      revision.map((_) => _.sql).join('\n\n'),
    )
    log.success(
      timed(`Created ${pc.cyan(`revisions/${doRevisionFileName}`)}`, start),
    )
  }
}

async function getPostgratorClient(driver: DatabaseDriver): Promise<
  | {
      driver: string
      execQuery: (query: string) => Promise<{ rows: any[] }>
    }
  | undefined
> {
  if (driver instanceof PostgresDriver) {
    const client = await driver.connect()
    if (client) {
      return {
        driver: 'pg',
        execQuery(query: string) {
          return driver.query(query)
        },
      }
    }
  } else if (driver instanceof SqliteDriver) {
    return {
      driver: 'sqlite3',
      async execQuery(query: string) {
        const stmt = driver.prepare(query)
        if (query.trim().toLowerCase().startsWith('select')) {
          const rows = stmt.all()
          return { rows }
        } else {
          const result = stmt.run()
          return {
            rows: [],
            changes: result.changes,
            lastInsertRowid: result.lastInsertRowid,
          }
        }
      },
    }
  }
}

function timed(log: string, globalStartOverride?: number) {
  return `${log} in ${pc.magenta(`${now(globalStartOverride)}ms`)}`
}

function now(globalStartOverride?: number) {
  return (perf.now() - (globalStartOverride ?? start)).toFixed(2)
}
